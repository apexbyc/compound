"""
ä¿®æ”¹ç‰ˆï¼šå¯»æ‰¾nå€¼ä½¿å¾—æ–°åˆ’åˆ†çš„å¤§å•/æ¼«é•¿è®¢å•é›†åˆå°½å¯èƒ½æ¥è¿‘åŸ10%åˆ†ä½æ•°åˆ’åˆ†
ç„¶ååŸºäºè¿™ä¸ªæ–°åˆ’åˆ†è®¡ç®—æ‰€æœ‰å› å­
"""

import pandas as pd
import numpy as np
from datetime import datetime, time
import warnings
from scipy.optimize import minimize_scalar

warnings.filterwarnings('ignore')


def find_optimal_n_for_stock(data, order_type='volume', target_percentile=0.9):
    """
    ä¸ºå•åªè‚¡ç¥¨å¯»æ‰¾æœ€ä¼˜nï¼Œä½¿å¾—å‡å€¼+n*æ ‡å‡†å·®åˆ’åˆ†ä¸åŸåˆ†ä½æ•°åˆ’åˆ†æœ€ç›¸ä¼¼

    å‚æ•°ï¼š
    data: è®¢å•æ•°æ®ï¼ˆæˆäº¤é‡æˆ–æˆäº¤æ—¶é•¿ï¼‰
    order_type: 'volume'ï¼ˆå¤§å•ï¼‰æˆ–'duration'ï¼ˆæ¼«é•¿è®¢å•ï¼‰
    target_percentile: åŸåˆ’åˆ†çš„åˆ†ä½æ•°ï¼ˆé»˜è®¤ä¸º0.9ï¼Œå³90%åˆ†ä½æ•°ï¼‰
    """
    if len(data) < 10:
        return 1.2816  # é»˜è®¤å€¼

    # åŸåˆ’åˆ†é˜ˆå€¼ï¼ˆ90%åˆ†ä½æ•°ï¼‰
    original_threshold = np.percentile(data, target_percentile * 100)

    # åŸåˆ’åˆ†ä¸‹çš„å¤§å•æ ‡ç­¾
    original_labels = data > original_threshold

    # ç›®æ ‡å‡½æ•°ï¼šæœ€å¤§åŒ–Jaccardç›¸ä¼¼åº¦
    def jaccard_similarity(n):
        # æ–°åˆ’åˆ†é˜ˆå€¼
        new_threshold = np.mean(data) + n * np.std(data)
        # æ–°åˆ’åˆ†æ ‡ç­¾
        new_labels = data > new_threshold
        # è®¡ç®—Jaccardç›¸ä¼¼åº¦
        intersection = np.sum(original_labels & new_labels)
        union = np.sum(original_labels | new_labels)
        if union == 0:
            return 0
        return intersection / union

    # è´Ÿç›¸ä¼¼åº¦ï¼ˆå› ä¸ºæœ€å°åŒ–ï¼‰
    def objective(n):
        return -jaccard_similarity(n)

    # æœç´¢nçš„èŒƒå›´ï¼ˆ-3åˆ°10ï¼Œä½†é€šå¸¸ä¸ºæ­£ï¼‰
    bounds = (-3, 10)

    try:
        result = minimize_scalar(objective, bounds=bounds, method='bounded')
        optimal_n = result.x

        # éªŒè¯æœ€ä¼˜nä¸‹çš„ç›¸ä¼¼åº¦
        similarity = jaccard_similarity(optimal_n)

        return optimal_n, similarity
    except:
        # å¦‚æœä¼˜åŒ–å¤±è´¥ï¼Œä½¿ç”¨ç®€å•æ–¹æ³•
        mean_val = np.mean(data)
        std_val = np.std(data)
        if std_val > 0:
            n_simple = (original_threshold - mean_val) / std_val
            return n_simple, jaccard_similarity(n_simple)
        else:
            return 1.2816, 0


def calculate_factors_with_optimal_n(data_path, output_dir=None):
    """
    1. ä¸ºæ¯åªè‚¡ç¥¨æ‰¾åˆ°æœ€ä¼˜nï¼Œä½¿å¾—æ–°åˆ’åˆ†æ¥è¿‘åŸ10%åˆ†ä½æ•°åˆ’åˆ†
    2. è®¡ç®—æ‰€æœ‰è‚¡ç¥¨çš„nå€¼ç»Ÿè®¡
    3. ä½¿ç”¨æœ€ä¼˜nï¼ˆå…¨å±€ï¼‰è®¡ç®—æ‰€æœ‰å› å­
    """
    print("=" * 80)
    print("ğŸ¯ å¯»æ‰¾æœ€ä¼˜nå€¼å¹¶è®¡ç®—å› å­ï¼ˆæ¥è¿‘åŸ10%åˆ†ä½æ•°åˆ’åˆ†ï¼‰")
    print("=" * 80)

    # 1. åŠ è½½æ•°æ®
    print("1. åŠ è½½æ•°æ®...")
    df = pd.read_parquet(data_path)

    # 2. æ—¶é—´å¤„ç†
    df['TradeTime'] = pd.to_datetime(df['date'].astype(str) + ' ' + df['Time'].astype(str).str[-15:])
    df_continuous = df[~df['TradeTime'].dt.time.between(time(9, 15), time(9, 30))].copy()

    # 3. æ”¶é›†æ‰€æœ‰è‚¡ç¥¨çš„nå€¼
    print("2. ä¸ºæ¯åªè‚¡ç¥¨è®¡ç®—æœ€ä¼˜nå€¼...")

    all_stocks = df_continuous['secucode'].unique()
    n_big_values = []
    n_long_values = []
    similarity_big_values = []
    similarity_long_values = []

    # è¿›åº¦è®¡æ•°å™¨
    progress_count = 0
    total_stocks = min(len(all_stocks), 50)  # æœ€å¤šå¤„ç†50åªè‚¡ç¥¨ä»¥å‡å°‘è®¡ç®—æ—¶é—´

    for stock in all_stocks[:total_stocks]:
        progress_count += 1
        if progress_count % 10 == 0:
            print(f"   è¿›åº¦: {progress_count}/{total_stocks}")

        stock_data = df_continuous[df_continuous['secucode'] == stock].copy()

        if len(stock_data) < 10:
            continue

        # è®¡ç®—è®¢å•ç‰¹å¾
        buy_orders = stock_data.groupby('BuyOrderID').agg(
            buy_volume=('Volume', 'sum'),
            buy_first_time=('TradeTime', 'min'),
            buy_last_time=('TradeTime', 'max')
        ).reset_index()
        buy_orders['buy_duration'] = (buy_orders['buy_last_time'] - buy_orders['buy_first_time']).dt.total_seconds()

        sell_orders = stock_data.groupby('SaleOrderID').agg(
            sell_volume=('Volume', 'sum'),
            sell_first_time=('TradeTime', 'min'),
            sell_last_time=('TradeTime', 'max')
        ).reset_index()
        sell_orders['sell_duration'] = (
                    sell_orders['sell_last_time'] - sell_orders['sell_first_time']).dt.total_seconds()

        # åˆå¹¶ä¹°å•å’Œå–å•æ•°æ®
        all_volumes = np.concatenate([buy_orders['buy_volume'].values, sell_orders['sell_volume'].values])
        all_durations = np.concatenate([buy_orders['buy_duration'].values, sell_orders['sell_duration'].values])

        # è¿‡æ»¤å¼‚å¸¸å€¼
        all_volumes = all_volumes[~np.isnan(all_volumes) & ~np.isinf(all_volumes)]
        all_durations = all_durations[~np.isnan(all_durations) & ~np.isinf(all_durations)]

        if len(all_volumes) >= 10:
            n_big, sim_big = find_optimal_n_for_stock(all_volumes, 'volume', 0.9)
            n_big_values.append(n_big)
            similarity_big_values.append(sim_big)

        if len(all_durations) >= 10:
            n_long, sim_long = find_optimal_n_for_stock(all_durations, 'duration', 0.9)
            n_long_values.append(n_long)
            similarity_long_values.append(sim_long)

    # 4. è®¡ç®—å…¨å±€nå€¼ï¼ˆä½¿ç”¨ä¸­ä½æ•°ï¼Œæ›´ç¨³å¥ï¼‰
    if len(n_big_values) > 0:
        global_n_big = np.median(n_big_values)
        global_n_long = np.median(n_long_values) if len(n_long_values) > 0 else 1.2816
    else:
        global_n_big = 1.2816
        global_n_long = 1.2816

    print("\n3. nå€¼è®¡ç®—ç»“æœ:")
    print("=" * 50)
    print(f"   å¤§å•nå€¼ç»Ÿè®¡:")
    print(f"     ä¸­ä½æ•°: {global_n_big:.6f}")
    if n_big_values:
        print(f"     å‡å€¼: {np.mean(n_big_values):.6f}")
        print(f"     æ ‡å‡†å·®: {np.std(n_big_values):.6f}")
        print(f"     èŒƒå›´: [{np.min(n_big_values):.6f}, {np.max(n_big_values):.6f}]")

    print(f"\n   æ¼«é•¿è®¢å•nå€¼ç»Ÿè®¡:")
    print(f"     ä¸­ä½æ•°: {global_n_long:.6f}")
    if n_long_values:
        print(f"     å‡å€¼: {np.mean(n_long_values):.6f}")
        print(f"     æ ‡å‡†å·®: {np.std(n_long_values):.6f}")
        print(f"     èŒƒå›´: [{np.min(n_long_values):.6f}, {np.max(n_long_values):.6f}]")

    print(f"\n   åˆ’åˆ†ç›¸ä¼¼åº¦ç»Ÿè®¡:")
    if similarity_big_values:
        print(f"     å¤§å•åˆ’åˆ†å¹³å‡Jaccardç›¸ä¼¼åº¦: {np.mean(similarity_big_values):.4f}")
    if similarity_long_values:
        print(f"     æ¼«é•¿è®¢å•åˆ’åˆ†å¹³å‡Jaccardç›¸ä¼¼åº¦: {np.mean(similarity_long_values):.4f}")

    # 5. ä½¿ç”¨å…¨å±€nå€¼è®¡ç®—æ‰€æœ‰è‚¡ç¥¨çš„å› å­
    print("\n4. ä½¿ç”¨å…¨å±€nå€¼è®¡ç®—æ‰€æœ‰è‚¡ç¥¨çš„å› å­...")

    results = []

    for stock in all_stocks:
        stock_data = df_continuous[df_continuous['secucode'] == stock].copy()

        if len(stock_data) < 10:
            continue

        total_volume = stock_data['Volume'].sum()

        # è®¡ç®—è®¢å•ç‰¹å¾
        buy_orders = stock_data.groupby('BuyOrderID').agg(
            buy_volume=('Volume', 'sum'),
            buy_first_time=('TradeTime', 'min'),
            buy_last_time=('TradeTime', 'max')
        ).reset_index()
        buy_orders['buy_duration'] = (buy_orders['buy_last_time'] - buy_orders['buy_first_time']).dt.total_seconds()

        sell_orders = stock_data.groupby('SaleOrderID').agg(
            sell_volume=('Volume', 'sum'),
            sell_first_time=('TradeTime', 'min'),
            sell_last_time=('TradeTime', 'max')
        ).reset_index()
        sell_orders['sell_duration'] = (
                    sell_orders['sell_last_time'] - sell_orders['sell_first_time']).dt.total_seconds()

        # è®¡ç®—é˜ˆå€¼ï¼ˆä½¿ç”¨å…¨å±€nï¼‰
        if len(buy_orders) > 0:
            buy_big_threshold = np.mean(buy_orders['buy_volume']) + global_n_big * np.std(buy_orders['buy_volume'])
            buy_long_threshold = np.mean(buy_orders['buy_duration']) + global_n_long * np.std(
                buy_orders['buy_duration'])
        else:
            buy_big_threshold = buy_long_threshold = 0

        if len(sell_orders) > 0:
            sell_big_threshold = np.mean(sell_orders['sell_volume']) + global_n_big * np.std(sell_orders['sell_volume'])
            sell_long_threshold = np.mean(sell_orders['sell_duration']) + global_n_long * np.std(
                sell_orders['sell_duration'])
        else:
            sell_big_threshold = sell_long_threshold = 0

        # åˆå¹¶ç‰¹å¾åˆ°æˆäº¤è®°å½•
        stock_data = stock_data.merge(
            buy_orders[['BuyOrderID', 'buy_volume', 'buy_duration']],
            on='BuyOrderID', how='left'
        )
        stock_data = stock_data.merge(
            sell_orders[['SaleOrderID', 'sell_volume', 'sell_duration']],
            on='SaleOrderID', how='left'
        )

        # æ ‡è®°è®¢å•å±æ€§
        stock_data['is_big_buy'] = stock_data['buy_volume'] > buy_big_threshold
        stock_data['is_big_sell'] = stock_data['sell_volume'] > sell_big_threshold
        stock_data['is_long_buy'] = stock_data['buy_duration'] > buy_long_threshold
        stock_data['is_long_sell'] = stock_data['sell_duration'] > sell_long_threshold

        # è®¡ç®—æˆäº¤é‡å æ¯”å‡½æ•°
        def volume_ratio(mask):
            return stock_data[mask]['Volume'].sum() / total_volume if total_volume > 0 else 0

        # ============================================
        # è®¡ç®—6ä¸ªåŸºæœ¬å­å› å­
        # ============================================
        big_buy_non_big_sell = volume_ratio(stock_data['is_big_buy'] & ~stock_data['is_big_sell'])
        non_big_buy_big_sell = volume_ratio(~stock_data['is_big_buy'] & stock_data['is_big_sell'])
        big_buy_big_sell = volume_ratio(stock_data['is_big_buy'] & stock_data['is_big_sell'])

        long_buy_non_long_sell = volume_ratio(stock_data['is_long_buy'] & ~stock_data['is_long_sell'])
        non_long_buy_long_sell = volume_ratio(~stock_data['is_long_buy'] & stock_data['is_long_sell'])
        long_buy_long_sell = volume_ratio(stock_data['is_long_buy'] & stock_data['is_long_sell'])

        # ============================================
        # è®¡ç®—4ä¸ªåˆæˆå› å­
        # ============================================
        volume_big_origin = big_buy_non_big_sell + non_big_buy_big_sell + 2 * big_buy_big_sell
        volume_big = (-big_buy_non_big_sell - non_big_buy_big_sell + big_buy_big_sell)
        volume_long = long_buy_non_long_sell + non_long_buy_long_sell + 2 * long_buy_long_sell
        volume_long_big = volume_big + volume_long

        # ============================================
        # è®¡ç®—16ç§è®¢å•ç±»å‹å› å­
        # ============================================
        order_type_factors = {}
        for bb in [0, 1]:
            for bs in [0, 1]:
                for lb in [0, 1]:
                    for ls in [0, 1]:
                        mask = (
                                (stock_data['is_big_buy'] == bool(bb)) &
                                (stock_data['is_big_sell'] == bool(bs)) &
                                (stock_data['is_long_buy'] == bool(lb)) &
                                (stock_data['is_long_sell'] == bool(ls))
                        )
                        key = f"BB{bb}_BS{bs}_LB{lb}_LS{ls}"
                        order_type_factors[key] = volume_ratio(mask)

        # ============================================
        # è®¡ç®—ç²¾é€‰å¤åˆå› å­
        # ============================================
        effective_factors = [
            order_type_factors['BB1_BS1_LB1_LS1'],  # æ­£æ–¹å‘
            order_type_factors['BB1_BS1_LB0_LS1'],  # æ­£æ–¹å‘
            order_type_factors['BB1_BS1_LB1_LS0'],  # æ­£æ–¹å‘
            -order_type_factors['BB0_BS1_LB0_LS1'],  # è´Ÿæ–¹å‘
            -order_type_factors['BB1_BS0_LB0_LS0']  # è´Ÿæ–¹å‘
        ]
        volume_long_big_select = np.mean(effective_factors)

        # ============================================
        # å­˜å‚¨ç»“æœ
        # ============================================
        result = {
            'secucode': stock,
            'total_volume': total_volume,
            'total_trades': len(stock_data),

            # é˜ˆå€¼ä¿¡æ¯
            'buy_big_threshold': buy_big_threshold,
            'sell_big_threshold': sell_big_threshold,
            'buy_long_threshold': buy_long_threshold,
            'sell_long_threshold': sell_long_threshold,

            # 6ä¸ªåŸºæœ¬å­å› å­
            'big_buy_non_big_sell': big_buy_non_big_sell,
            'non_big_buy_big_sell': non_big_buy_big_sell,
            'big_buy_big_sell': big_buy_big_sell,
            'long_buy_non_long_sell': long_buy_non_long_sell,
            'non_long_buy_long_sell': non_long_buy_long_sell,
            'long_buy_long_sell': long_buy_long_sell,

            # 4ä¸ªåˆæˆå› å­
            'VolumeBigOrigin': volume_big_origin,
            'VolumeBig': volume_big,
            'VolumeLong': volume_long,
            'VolumeLongBig': volume_long_big,

            # ç²¾é€‰å¤åˆå› å­
            'VolumeLongBigSelect': volume_long_big_select,

            # å¤§å•æ¯”ä¾‹ï¼ˆæ–°åˆ’åˆ†ä¸‹ï¼‰
            'big_order_ratio_buy': np.mean(stock_data['is_big_buy']),
            'big_order_ratio_sell': np.mean(stock_data['is_big_sell']),
            'long_order_ratio_buy': np.mean(stock_data['is_long_buy']),
            'long_order_ratio_sell': np.mean(stock_data['is_long_sell']),
        }

        # æ·»åŠ 16ç§è®¢å•ç±»å‹å› å­
        for key, value in order_type_factors.items():
            result[key] = value

        results.append(result)

    # è½¬æ¢ä¸ºDataFrame
    factors_df = pd.DataFrame(results)

    # 6. éªŒè¯æ•°å­¦å…³ç³»
    print("\n5. éªŒè¯å› å­è®¡ç®—æ­£ç¡®æ€§...")

    if len(factors_df) == 0:
        print("âŒ æ²¡æœ‰è®¡ç®—åˆ°ä»»ä½•å› å­")
        return None, pd.DataFrame()

    # éªŒè¯å¤åˆå› å­å…¬å¼
    factors_df['éªŒè¯_å¤åˆå› å­'] = factors_df['VolumeBig'] + factors_df['VolumeLong']
    diff_composite = (factors_df['VolumeLongBig'] - factors_df['éªŒè¯_å¤åˆå› å­']).abs().max()
    print(f"   âœ… å¤åˆå› å­éªŒè¯è¯¯å·®: {diff_composite:.10f}")

    # éªŒè¯ä¼ ç»Ÿå¤§å•å› å­å…¬å¼
    factors_df['éªŒè¯_ä¼ ç»Ÿå¤§å•'] = (
            factors_df['big_buy_non_big_sell'] +
            factors_df['non_big_buy_big_sell'] +
            2 * factors_df['big_buy_big_sell']
    )
    diff_origin = (factors_df['VolumeBigOrigin'] - factors_df['éªŒè¯_ä¼ ç»Ÿå¤§å•']).abs().max()
    print(f"   âœ… ä¼ ç»Ÿå¤§å•å› å­éªŒè¯è¯¯å·®: {diff_origin:.10f}")

    # éªŒè¯16ç§è®¢å•ç±»å‹ä¹‹å’Œä¸º1
    order_type_cols = [c for c in factors_df.columns if c.startswith('BB')]
    factors_df['è®¢å•ç±»å‹æ€»å’Œ'] = factors_df[order_type_cols].sum(axis=1)
    diff_order_types = (factors_df['è®¢å•ç±»å‹æ€»å’Œ'] - 1).abs().max()
    print(f"   âœ… 16ç§è®¢å•ç±»å‹æ€»å’ŒéªŒè¯: {diff_order_types:.10f}")

    # 7. ç»Ÿè®¡æ–°åˆ’åˆ†ä¸‹çš„å¤§å•æ¯”ä¾‹
    print("\n6. æ–°åˆ’åˆ†ä¸‹çš„å¤§å•/æ¼«é•¿è®¢å•æ¯”ä¾‹ç»Ÿè®¡:")
    print(
        f"   å¤§ä¹°å•å¹³å‡æ¯”ä¾‹: {factors_df['big_order_ratio_buy'].mean():.4f} Â± {factors_df['big_order_ratio_buy'].std():.4f}")
    print(
        f"   å¤§å–å•å¹³å‡æ¯”ä¾‹: {factors_df['big_order_ratio_sell'].mean():.4f} Â± {factors_df['big_order_ratio_sell'].std():.4f}")
    print(
        f"   æ¼«é•¿ä¹°å•å¹³å‡æ¯”ä¾‹: {factors_df['long_order_ratio_buy'].mean():.4f} Â± {factors_df['long_order_ratio_buy'].std():.4f}")
    print(
        f"   æ¼«é•¿å–å•å¹³å‡æ¯”ä¾‹: {factors_df['long_order_ratio_sell'].mean():.4f} Â± {factors_df['long_order_ratio_sell'].std():.4f}")

    # 8. ä¿å­˜å…¨å±€nå€¼
    global_n_values = {
        'global_n_big': global_n_big,
        'global_n_long': global_n_long,
        'n_big_values_count': len(n_big_values),
        'n_long_values_count': len(n_long_values),
        'avg_similarity_big': np.mean(similarity_big_values) if similarity_big_values else 0,
        'avg_similarity_long': np.mean(similarity_long_values) if similarity_long_values else 0,
    }

    return global_n_values, factors_df


def save_results(global_n_values, factors_df, output_dir):
    """ä¿å­˜nå€¼å’Œå› å­ç»“æœ"""
    import os

    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    # 1. ä¿å­˜nå€¼
    n_df = pd.DataFrame([global_n_values])
    n_path = os.path.join(output_dir, f"optimal_n_values_{timestamp}.csv")
    n_df.to_csv(n_path, index=False, encoding='utf-8-sig')
    print(f"\nğŸ’¾ æœ€ä¼˜nå€¼å·²ä¿å­˜åˆ°: {n_path}")

    # 2. ä¿å­˜è¯¦ç»†nå€¼ç»Ÿè®¡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if 'n_big_values' in global_n_values:
        n_detailed = {
            'metric': ['ä¸­ä½æ•°', 'å‡å€¼', 'æ ‡å‡†å·®', 'æœ€å°å€¼', 'æœ€å¤§å€¼'],
            'n_big': [
                np.median(global_n_values['n_big_values']),
                np.mean(global_n_values['n_big_values']),
                np.std(global_n_values['n_big_values']),
                np.min(global_n_values['n_big_values']),
                np.max(global_n_values['n_big_values'])
            ],
            'n_long': [
                np.median(global_n_values['n_long_values']),
                np.mean(global_n_values['n_long_values']),
                np.std(global_n_values['n_long_values']),
                np.min(global_n_values['n_long_values']),
                np.max(global_n_values['n_long_values'])
            ]
        }
        n_detailed_df = pd.DataFrame(n_detailed)
        n_detailed_path = n_path.replace('.csv', '_detailed.csv')
        n_detailed_df.to_csv(n_detailed_path, index=False, encoding='utf-8-sig')
        print(f"ğŸ’¾ è¯¦ç»†nå€¼ç»Ÿè®¡å·²ä¿å­˜åˆ°: {n_detailed_path}")

    # 3. ä¿å­˜æ ¸å¿ƒå› å­
    core_cols = [
        'secucode', 'total_volume', 'total_trades',
        'VolumeBigOrigin', 'VolumeBig', 'VolumeLong',
        'VolumeLongBig', 'VolumeLongBigSelect',
        'big_order_ratio_buy', 'big_order_ratio_sell'
    ]
    core_cols = [c for c in core_cols if c in factors_df.columns]

    core_path = os.path.join(output_dir, f"core_factors_optimal_n_{timestamp}.csv")
    factors_df[core_cols].to_csv(core_path, index=False, encoding='utf-8-sig')
    print(f"ğŸ’¾ æ ¸å¿ƒå› å­å·²ä¿å­˜åˆ°: {core_path}")

    # 4. ä¿å­˜å®Œæ•´å› å­
    full_path = os.path.join(output_dir, f"all_factors_optimal_n_{timestamp}.csv")
    factors_df.to_csv(full_path, index=False, encoding='utf-8-sig')
    print(f"ğŸ’¾ å®Œæ•´å› å­å·²ä¿å­˜åˆ°: {full_path}")

    # 5. ä¿å­˜16ç§è®¢å•ç±»å‹
    order_type_cols = [c for c in factors_df.columns if c.startswith('BB')]
    order_type_cols = ['secucode'] + order_type_cols

    order_path = os.path.join(output_dir, f"order_types_optimal_n_{timestamp}.csv")
    factors_df[order_type_cols].to_csv(order_path, index=False, encoding='utf-8-sig')
    print(f"ğŸ’¾ 16ç§è®¢å•ç±»å‹å·²ä¿å­˜åˆ°: {order_path}")

    return {
        'n_values': n_path,
        'core_factors': core_path,
        'all_factors': full_path,
        'order_types': order_path
    }


def compare_with_original_factors(factors_df_new, factors_df_original=None):
    """
    æ¯”è¾ƒæ–°åˆ’åˆ†å› å­ä¸åŸåˆ’åˆ†å› å­çš„å·®å¼‚
    å¦‚æœæ²¡æœ‰åŸåˆ’åˆ†å› å­æ•°æ®ï¼Œåªæ˜¾ç¤ºæ–°åˆ’åˆ†çš„ç»“æœ
    """
    print("\n" + "=" * 80)
    print("ğŸ“Š æ–°åˆ’åˆ†å› å­ç»“æœæ‘˜è¦")
    print("=" * 80)

    print(f"è‚¡ç¥¨æ•°é‡: {len(factors_df_new)}")

    # æ ¸å¿ƒå› å­ç»Ÿè®¡
    core_factors = ['VolumeBig', 'VolumeLong', 'VolumeLongBig', 'VolumeLongBigSelect']

    print(f"\næ ¸å¿ƒå› å­ç»Ÿè®¡ï¼ˆæ–°åˆ’åˆ†ï¼‰:")
    for factor in core_factors:
        if factor in factors_df_new.columns:
            mean_val = factors_df_new[factor].mean()
            std_val = factors_df_new[factor].std()
            min_val = factors_df_new[factor].min()
            max_val = factors_df_new[factor].max()
            print(f"  {factor}: å‡å€¼={mean_val:.4f}, æ ‡å‡†å·®={std_val:.4f}, èŒƒå›´=[{min_val:.4f}, {max_val:.4f}]")

    # æ˜¾ç¤ºå‰5åªè‚¡ç¥¨çš„å› å­å€¼
    print(f"\nğŸ“‹ å‰5åªè‚¡ç¥¨çš„å› å­å€¼ï¼ˆæ–°åˆ’åˆ†ï¼‰:")
    display_cols = ['secucode', 'VolumeBig', 'VolumeLong', 'VolumeLongBig', 'VolumeLongBigSelect']
    display_cols = [c for c in display_cols if c in factors_df_new.columns]
    print(factors_df_new[display_cols].head().round(4))

    return factors_df_new


# ä¸»ç¨‹åº
if __name__ == "__main__":
    print("=" * 80)
    print("ğŸ¦ å¯»æ‰¾æœ€ä¼˜nå€¼å¹¶è®¡ç®—å› å­ï¼ˆæ¥è¿‘åŸ10%åˆ†ä½æ•°åˆ’åˆ†ï¼‰")
    print("=" * 80)

    try:
        # è¾“å…¥æ–‡ä»¶è·¯å¾„
        data_path = "D:/pycharm/pythonProject/dataExample.parquet"

        # è¾“å‡ºç›®å½•
        output_dir = "D:/pycharm/pythonProject/optimal_n_factors"

        print(f"ğŸ“ è¾“å…¥æ–‡ä»¶: {data_path}")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")

        # è®¡ç®—æœ€ä¼˜nå€¼å’Œå› å­
        global_n_values, factors_df = calculate_factors_with_optimal_n(data_path, output_dir)

        if factors_df is not None and len(factors_df) > 0:
            # æ˜¾ç¤ºå…³é”®ç»“æœ
            print("\n" + "=" * 80)
            print("ğŸ¯ è®¡ç®—å®Œæˆï¼")
            print("=" * 80)

            print(f"\nğŸ“Š å…¨å±€æœ€ä¼˜nå€¼:")
            print(f"   å¤§å•åˆ’åˆ† n_big: {global_n_values['global_n_big']:.6f}")
            print(f"   æ¼«é•¿è®¢å•åˆ’åˆ† n_long: {global_n_values['global_n_long']:.6f}")

            if 'avg_similarity_big' in global_n_values:
                print(f"   å¤§å•åˆ’åˆ†å¹³å‡ç›¸ä¼¼åº¦: {global_n_values['avg_similarity_big']:.4f}")
                print(f"   æ¼«é•¿è®¢å•åˆ’åˆ†å¹³å‡ç›¸ä¼¼åº¦: {global_n_values['avg_similarity_long']:.4f}")

            # æ¯”è¾ƒå› å­ç»“æœ
            factors_df = compare_with_original_factors(factors_df)

            # ä¿å­˜ç»“æœ
            file_paths = save_results(global_n_values, factors_df, output_dir)

            print(f"\nâœ… æ‰€æœ‰è®¡ç®—å®Œæˆï¼")
            print(f"   è‚¡ç¥¨æ•°é‡: {len(factors_df)}")
            print(f"   è¾“å‡ºæ–‡ä»¶å·²ä¿å­˜åˆ°: {output_dir}")

        else:
            print("\nâŒ æ²¡æœ‰è®¡ç®—åˆ°ä»»ä½•å› å­ï¼Œè¯·æ£€æŸ¥æ•°æ®æ ¼å¼")

    except Exception as e:
        print(f"\nâŒ ç¨‹åºæ‰§è¡Œå‡ºé”™: {str(e)}")
        import traceback

        traceback.print_exc()